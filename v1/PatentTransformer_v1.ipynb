{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PatentTransformer-v1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2dMuHFhsQEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "\n",
        "proj_folder = '/content/gpt-2'\n",
        "git_src = 'https://github.com/openai/gpt-2' \n",
        "if not os.path.exists(proj_folder):\n",
        "  !git clone $git_src\n",
        "else:\n",
        "  print('existed: %s' % proj_folder)\n",
        "  os.chdir(proj_folder)  \n",
        "  !git pull origin master\n",
        "\n",
        "os.chdir(proj_folder)\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZJwuWIJsQI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "print('tf version: %s' % tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if 'GPU' in device_name:\n",
        "  print('GPU ready: %s' % device_name) \n",
        "  GPU_FLAG = True\n",
        "else:\n",
        "  print('CPU only.....')    \n",
        "\n",
        "src_path = '/content/gpt-2/src'\n",
        "if src_path not in sys.path:\n",
        "  sys.path += [src_path]\n",
        "\n",
        "os.chdir(proj_folder)\n",
        "model_name= '345M'  \n",
        "if os.path.exists(os.path.join('models', model_name)) == False:\n",
        "  print('download model %s....' % model_name)\n",
        "  !PYTHONPATH=src; python ./download_model.py $model_name\n",
        "else:\n",
        "  print('existed: model %s' % model_name)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9C9lZ6jgrCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the following code is copied from: \n",
        "# https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive/39225039#39225039\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    def get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_response_content(response, destination):\n",
        "        CHUNK_SIZE = 32768\n",
        "\n",
        "        with open(destination, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "    session = requests.Session()\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "    save_response_content(response, destination)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI-rkoeYTWFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# donwload fine-tuned model for patents\n",
        "\n",
        "ckpt_path = 'saved_checkpoint'\n",
        "if os.path.exists(ckpt_path):\n",
        "  print('Existed: %s' % ckpt_path)\n",
        "  !ls $ckpt_path\n",
        "else:\n",
        "  os.mkdir(ckpt_path)\n",
        "  os.chdir(ckpt_path)\n",
        "  print('Downloading files to %s....' % ckpt_path)\n",
        "  download_file_from_google_drive('10Qc8SIuwq6w6guwDnpdNzKH24NxZ3oG-', 'checkpoint')\n",
        "  download_file_from_google_drive('1KDO8ikS5IJqo1S6zfbOnFae3-zS6-8f0', 'model-945000.meta')\n",
        "  download_file_from_google_drive('1Z0PnW0BHFApZBBWL3SGDzwgFme1mwFwi', 'model-945000.data-00000-of-00001')\n",
        "  download_file_from_google_drive('1ipOdKVOUM8h45njLLtbhJFbfqTNFLNq_', 'model-945000.index')\n",
        "  !ls -al \n",
        "  print('Download: ok')\n",
        "os.chdir(proj_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiBKgo1SsQWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the following is my enchancement based on: \n",
        "# https://github.com/openai/gpt-2/blob/master/src/sample.py\n",
        "# https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import model, sample, encoder\n",
        "\n",
        "def generate_text(seed_text, sess, output, context, nsamples, batch_size):\n",
        "    generated = 0\n",
        "    for _ in range(nsamples // batch_size):\n",
        "      out = sess.run(output, feed_dict={\n",
        "          context: [context_tokens for _ in range(batch_size)]\n",
        "      })[:, len(context_tokens):]\n",
        "      for i in range(batch_size):\n",
        "        generated += 1\n",
        "        text = enc.decode(out[i])\n",
        "        text = till_end_of_text(text)\n",
        "        text = seed_text + ' ' + text\n",
        "        print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "        text = text.replace(tag_start, '').strip()\n",
        "        span_text = text.replace(' @@@ ','\\n    ')\n",
        "        print(span_text)\n",
        "    print(\"=\" * 80)\n",
        "    print('\\n')\n",
        "\n",
        "def dynamic_kp_logits(logits, top_kp):\n",
        "    k = 1000 # observe probability of the top n \n",
        "    probs_logits = tf.nn.softmax(logits)\n",
        "    k_probs, _ = tf.nn.top_k(probs_logits, k=k)\n",
        "    k_probs = tf.squeeze(k_probs)\n",
        "    probs_max = tf.reduce_max(k_probs)\n",
        "    k_threshold = tf.multiply(probs_max, top_kp)\n",
        "    probs_mask = tf.to_int32(k_probs >= k_threshold)\n",
        "    num_of_k = tf.count_nonzero(probs_mask, dtype=tf.float32) \n",
        "\n",
        "    values, _ = tf.nn.top_k(logits, k=tf.cast(num_of_k, dtype=tf.int32))\n",
        "    min_values = values[:, -1, tf.newaxis]\n",
        "    result_logits = tf.where(\n",
        "        logits < min_values,\n",
        "        tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
        "        logits,)\n",
        "\n",
        "    return result_logits\n",
        "\n",
        "def sample_sequence_kp(*, hparams, length, start_token=None, batch_size=None, \n",
        "                       context=None, temperature=1, top_kp=0.1):\n",
        "    if start_token is None:\n",
        "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
        "    else:\n",
        "        assert context is None, 'Specify exactly one of start_token and context!'\n",
        "        context = tf.fill([batch_size, 1], start_token)\n",
        "\n",
        "    def step(hparams, tokens, past=None):\n",
        "        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
        "        presents = lm_output['present']\n",
        "        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'presents': presents,\n",
        "        }\n",
        "\n",
        "    with tf.name_scope('sample_sequence'):\n",
        "        def body(past, prev, output):\n",
        "            next_outputs = step(hparams, prev, past=past)\n",
        "            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n",
        "            logits = dynamic_kp_logits(logits, top_kp)\n",
        "            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n",
        "            return [\n",
        "                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
        "                samples,\n",
        "                tf.concat([output, samples], axis=1)\n",
        "            ]\n",
        "\n",
        "        past, prev, output = body(None, context, context)\n",
        "\n",
        "        def cond(*args):\n",
        "            return True\n",
        "\n",
        "        _, _, tokens = tf.while_loop(\n",
        "            cond=cond, body=body,\n",
        "            maximum_iterations=length - 1,\n",
        "            loop_vars=[\n",
        "                past,\n",
        "                prev,\n",
        "                output\n",
        "            ],\n",
        "            shape_invariants=[\n",
        "                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "            ],\n",
        "            back_prop=False,\n",
        "        )\n",
        "\n",
        "        return tokens\n",
        "\n",
        "def till_end_of_text(text):\n",
        "  tag_end = '<|endoftext|>'\n",
        "  result = ''\n",
        "  pos2 = text.find(tag_end)\n",
        "  if pos2 == -1:\n",
        "    result = '(unable to generate....probably not patent text)'\n",
        "  elif pos2 == 0:\n",
        "    result = '(end of patent claim)'\n",
        "  else:\n",
        "    result = text[:pos2].strip()\n",
        "\n",
        "  return result\n",
        "\n",
        "# main program\n",
        "model_name='345M'\n",
        "seed=None\n",
        "nsamples=1\n",
        "batch_size=1\n",
        "length=None\n",
        "temperature=1\n",
        "top_k=40\n",
        "top_p=0.9\n",
        "top_kp=0.1\n",
        "\n",
        "tag_start = '<|startoftext|>'\n",
        "\n",
        "models_dir = 'models'\n",
        "models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "if batch_size is None:\n",
        "  batch_size = 1\n",
        "assert nsamples % batch_size == 0\n",
        "\n",
        "enc = encoder.get_encoder(model_name, models_dir)\n",
        "hparams = model.default_hparams()\n",
        "with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "  hparams.override_from_dict(json.load(f))\n",
        "\n",
        "if length is None:\n",
        "  length = hparams.n_ctx // 2\n",
        "elif length > hparams.n_ctx:\n",
        "  raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "with tf.Session(graph=tf.Graph()) as sess:\n",
        "  context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "  np.random.seed(seed)\n",
        "  tf.set_random_seed(seed)\n",
        "\n",
        "  # original sampling in GPT-2\n",
        "  output_top_k = sample.sample_sequence(\n",
        "    hparams=hparams, length=length,\n",
        "    context=context,\n",
        "    batch_size=batch_size,\n",
        "    temperature=temperature, top_k=top_k, top_p=1)\n",
        "  output_top_p = sample.sample_sequence(\n",
        "    hparams=hparams, length=length,\n",
        "    context=context,\n",
        "    batch_size=batch_size,\n",
        "    temperature=temperature, top_k=0, top_p=top_p)\n",
        "  \n",
        "  # a different sampling in our research\n",
        "  output_top_kp = sample_sequence_kp(\n",
        "    hparams=hparams, length=length,\n",
        "    context=context,\n",
        "    batch_size=batch_size,\n",
        "    temperature=temperature, top_kp=top_kp)\n",
        "\n",
        "  saver = tf.train.Saver()\n",
        "\n",
        "  # use the fine-tuned model for patents\n",
        "  ckpt = tf.train.latest_checkpoint(ckpt_path)\n",
        "\n",
        "  # original model released by OpenAI\n",
        "  # ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "\n",
        "  saver.restore(sess, ckpt)\n",
        "  while True:\n",
        "    print('Input text or \"exit\" or \"Enter\" key for unconditional sampling.....')\n",
        "    seed_text = input(\">>> \")\n",
        "    if seed_text == 'exit':\n",
        "      break\n",
        "    if seed_text == '':\n",
        "      seed_text = tag_start \n",
        "    context_tokens = enc.encode(seed_text)\n",
        "    print('top_k = %s' % top_k)\n",
        "    generate_text(seed_text, sess, output_top_k, context, nsamples, batch_size)\n",
        "    print('top_p = %s' % top_p)\n",
        "    generate_text(seed_text, sess, output_top_p, context, nsamples, batch_size)\n",
        "    print('top_kp = %s' % top_kp)\n",
        "    generate_text(seed_text, sess, output_top_kp, context, nsamples, batch_size)\n",
        "\n",
        "  print('Thank you.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-ikNAbesQSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}