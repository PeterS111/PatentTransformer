# -*- coding: utf-8 -*-
"""(paper9) (dev, gdrive) PatentTransformer-v2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IXcOMeXxkqHcXlPuVM3u972JZh081k45

### Augmented Inventing
"""

import sys

pretrained_model = 'M2' 

# M1: small model for 1976~2016
# M2: medium model for 1976~2016
# M3: small model for 2016
# M4: medium model for 2016

if pretrained_model in ['M1', 'M3']:
  model_name= '124M'
elif pretrained_model in ['M2', 'M4']:
  model_name= '355M'
else:
  print('unknown mode: %s' % pretrained_model)
  sys.exit(1)

import os

proj_folder = '/content/gpt-2'
git_src = 'https://github.com/openai/gpt-2' 
if not os.path.exists(proj_folder):
  !git clone $git_src
else:
  print('existed: %s' % proj_folder)
  os.chdir(proj_folder)  
  !git pull origin master

os.chdir(proj_folder)
!pip3 install -r requirements.txt

import tensorflow as tf

print('tf version: %s' % tf.__version__)
device_name = tf.test.gpu_device_name()
if 'GPU' in device_name:
  print('GPU ready: %s' % device_name) 
  GPU_FLAG = True
else:
  print('CPU only.....')    

src_path = '/content/gpt-2/src'
if src_path not in sys.path:
  sys.path += [src_path]

os.chdir(proj_folder)
if os.path.exists(os.path.join('models', model_name)) == False:
  print('download model %s....' % model_name)
  !PYTHONPATH=src; python ./download_model.py $model_name
else:
  print('existed: model %s' % model_name)

# the following code is copied from: 
# https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive/39225039#39225039

import requests

def download_file_from_google_drive(id, destination):
    def get_confirm_token(response):
        for key, value in response.cookies.items():
            if key.startswith('download_warning'):
                return value

        return None

    def save_response_content(response, destination):
        CHUNK_SIZE = 32768

        with open(destination, "wb") as f:
            for chunk in response.iter_content(CHUNK_SIZE):
                if chunk: # filter out keep-alive new chunks
                    f.write(chunk)

    URL = "https://docs.google.com/uc?export=download"
    session = requests.Session()
    response = session.get(URL, params = { 'id' : id }, stream = True)
    token = get_confirm_token(response)
    if token:
        params = { 'id' : id, 'confirm' : token }
        response = session.get(URL, params = params, stream = True)
    save_response_content(response, destination)

# donwload fine-tuned model for patents

download_links = {
  'M1':{
    'checkpoint': '1F6YxlYHy-0f0vmsEi5gOe91wLBCxk6Ud',
    'model.ckpt-1000000.index': '1KqSe-N9sW2hZEY7-M7RSwU0_CfK1sSDI',
    'model.ckpt-1000000.meta': '1F21mkrvf3bfwx-2YZlT46_qU3Vgva7NI',
    'model.ckpt-1000000.data-00000-of-00001': '1N4AI0N930-3heAUP2f5_pjuZQYC62J4w'
  },
  'M2':{
    'checkpoint': '1eqWbBY03soS2uurKYdbj1GEdOvCAtbw9',
    'model.ckpt-1000000.index': '1X96DuunLroV7CP4GJJpMuWxqtP0HxH9n',
    'model.ckpt-1000000.meta': '1FoabPlD9Ek4vuTer2FVG7TLZCrKmdW1B',
    'model.ckpt-1000000.data-00000-of-00001': '1AhU1Cx2_hpzYk3Wrsz7G36qOXWvOJfbY'
  },
  'M3':{
    'checkpoint': '1g8F5Ju4-6QYXeTa74BxS4w_dv38bBWwS',
    'model.ckpt-1000000.index': '1MLeMzyNdDnxTWoFdb6eacAqrv-EH79Hh',
    'model.ckpt-1000000.meta': '1vCSneihB3As9rF0X6LoWltb7GKHIZEML',
    'model.ckpt-1000000.data-00000-of-00001': '1IbrnX1fvSpuMoYVsjrBnMNTYnnTkukso'
  },
  'M4':{
    'checkpoint': '16pNZTOS-YMlGZGSapMuhduLwrrIPRD7E',
    'model.ckpt-1000000.index': '1TFIKqujh1s6285ShaN7QjNgNfm62qMp4',
    'model.ckpt-1000000.meta': '1jEFYMNVPgId70Hkuy1yXdnUMzTXwC7XY',
    'model.ckpt-1000000.data-00000-of-00001': '1FzesRtEhz7cu1ZxCbwQRsUd8uFuFyWmH'
  },
}

ckpt_path = 'saved_checkpoint_%s' % model_name
if os.path.exists(ckpt_path):
  print('Existed: %s' % ckpt_path)
  !ls $ckpt_path
else:
  os.mkdir(ckpt_path)
  os.chdir(ckpt_path)
  print('Downloading files to %s....' % ckpt_path)
  for k, v in download_links[pretrained_model].items():
    download_file_from_google_drive(v, k)
  !ls -al 
  print('Download: ok')
os.chdir(proj_folder)

import json
import os
import numpy as np

import model, sample, encoder

foward_start_tags = {'title':'<|startoftitle|>', \
                     'abstract':'<|startofabstract|>', \
                     'claim': '<|startoftext|>', \
                     'dep': '<|startoftext|>'}
foward_end_tags = {'title':'<|endoftitle|>', \
                   'abstract':'<|endofabstract|>', \
                   'claim': '<|endoftext|>', \
                   'dep': '<|startoftext|>'}
backward_start_tags = {'title':'<|backwardtitlestart>', \
                     'abstract':'<|backwardabstractstart>', \
                     'claim': '<|startofbackward|>'}
backward_end_tags = {'title':'<|backwardtitleend|>', \
                   'abstract':'<|backwardabstractend|>', \
                   'claim': '<|endofbackward|>'}

# text2text mapping
tag_title2abstract = '<|title2abstract|>'
tag_abstract2title = '<|abstract2title|>'
tag_abstract2claim = '<|abstract2claim|>'
tag_claim2abstract = '<|claim2abstract|>'
dep_separator = '<|dep|>'

def generate_output(context, count, num_of_generation, sess, text, 
                       sampler, enc, batch_size, cut_tag):
  results = []

  # forward
  text = text.strip()
  context_tokens = enc.encode(text)

  out = sess.run(sampler, feed_dict={
      context: [context_tokens for _ in range(batch_size)]
  })[:, len(context_tokens):]
  
  for i in range(batch_size):
    text = enc.decode(out[i])
    pos = text.find(cut_tag)
    if pos >= 0:
      text = text[:pos].strip()
    if text == '':
      continue
      
    results.append(text)
    count += 1
    if count >= num_of_generation:
      break
      
  return results  

def text2text_mapping(input_text, mapping, gen_count=1):
  all_results = []
  if mapping == 'dep':
    meta1 = meta2 = 'claim'
    print('[ dependent claim ]')
  else:
    meta1, meta2 = mapping.split('2')
    print('[ %s --> %s ]' % (meta1, meta2))
  raw_text = ''

  count = 0 
  raw_text = ' '.join([foward_start_tags[meta1], input_text, \
    foward_end_tags[meta1]]) 
  raw_text += ' <|' + mapping + '|> ' + foward_start_tags[meta2]
  while count < gen_count:
    batch_results = generate_output(context, count, 
      gen_count, sess, raw_text, sampler, enc, 
      batch_size, foward_end_tags[meta2])
    count += len(batch_results)
    all_results += batch_results

  for i, row in enumerate(all_results):
    row = row.replace('<|span|>', '\n\t')
    print('%s' % row) 
    #print('[ %s ] %s' % (i, row))
  print('')

  return all_results

def patent_text_gen(input_text, metadata, direction='forward', gen_count=1):
  all_results = []

  print('[ %s ] direction=%s, input_text=%s' % (metadata, direction, input_text))
  count = 0 
  if direction == 'forward':
    raw_text = foward_start_tags[metadata] + ' ' + input_text
    while count < gen_count:
      batch_results = generate_output(context, count, 
        gen_count, sess, raw_text, sampler, enc, 
        batch_size, foward_end_tags[metadata])
      count += len(batch_results)
      for i, row in enumerate(batch_results):
        s = input_text + ' ' + row
        all_results.append(s.strip())
  elif direction == 'backward':
    reversed_text = ' '.join(input_text.split()[::-1])
    raw_text = backward_end_tags[metadata] + ' ' + reversed_text
    while count < gen_count:
      batch_results = generate_output(context, count, 
        gen_count, sess, raw_text, sampler, enc, 
        batch_size, backward_start_tags[metadata])
      count += len(batch_results)       
      for i, row in enumerate(batch_results):
        reversed_row = ' '.join(row.split()[::-1])
        all_results.append(reversed_row + ' ' + input_text)
  elif direction == 'both':
    raw_text = foward_start_tags[metadata] + ' ' + input_text
    # forward
    while count < gen_count:
      batch_results = generate_output(context, count, 
        gen_count, sess, raw_text, sampler, enc, 
        batch_size, foward_end_tags[metadata])
      count += len(batch_results) 
      for i, row in enumerate(batch_results):
        all_results.append(input_text + ' ' + row)

    # backward, generate one by one
    for i, one_record in enumerate(all_results):
      reversed_text = ' '.join(one_record.split()[::-1])
      raw_text = backward_end_tags[metadata] + ' ' + reversed_text
      batch_results = generate_output(context, count, 
        1, sess, raw_text, sampler, enc, 
        batch_size, backward_start_tags[metadata])
      reversed_result = ' '.join(batch_results[0].split()[::-1])
      all_results[i] = reversed_result + ' ' + one_record
  else: 
    print('unknown direction: %s' % direction)
  
  for i, row in enumerate(all_results):
    print('%s' % row)
    #print('[ %s ] %s' % (i, row))
  print('')

  return all_results

# the following is my enchancement based on: 
# https://github.com/openai/gpt-2/blob/master/src/sample.py
# https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py

# input_text: a few words 
# metadata: title / abstract / claim 
# text2text_mapping: title2abstract / abstract2title / abstract2claim /
#                    claim2abstract
# direction: forward / backward / both
# gen_count: how many records to generate

seed=None
nsamples=1
batch_size=1
length=None
temperature=1
top_k=40

models_dir = 'models'
models_dir = os.path.expanduser(os.path.expandvars(models_dir))
if batch_size is None:
  batch_size = 1
assert nsamples % batch_size == 0

enc = encoder.get_encoder(model_name, models_dir)
hparams = model.default_hparams()
with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:
  hparams.override_from_dict(json.load(f))

if length is None:
  length = hparams.n_ctx // 2
elif length > hparams.n_ctx:
  raise ValueError("Can't get samples longer than window size: %s" % hparams.n_ctx)

sess = tf.InteractiveSession() 
context = tf.placeholder(tf.int32, [batch_size, None])
sampler = sample.sample_sequence(
  hparams=hparams, length=length,
  context=context,
  batch_size=batch_size,
  temperature=temperature, top_k=top_k
)
saver = tf.train.Saver()
ckpt = tf.train.latest_checkpoint(ckpt_path)
saver.restore(sess, ckpt)

#seed_text = 'temperature optimization'
while True:
  print('Demo: a few words --> title --> abstract --> independent claim --> dependent claims')
  print('Input text or "exit" or "Enter" key for unconditional sampling.....')
  seed_text = input(">>> ")
  direction = 'both'
  if seed_text == 'exit':
    break
  if seed_text == '':
    direction = 'forward'

  # from a few words to a patent title
  outputs = patent_text_gen(input_text=seed_text, metadata='title', 
                            direction=direction, gen_count=1)

  # from the patent title to a patent abstract
  results = text2text_mapping(input_text=outputs[0], mapping='title2abstract', gen_count=1)

  # from the patent abstract to an independent claim
  results = text2text_mapping(input_text=outputs[0], mapping='abstract2claim', gen_count=1)

  # from the independent claim to two dependent claims
  results = text2text_mapping(input_text=outputs[0], mapping='dep', gen_count=2)
  
print('Thank you for testing Augmented Inventing.')

